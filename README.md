# PsyThera-LLM ‚Äî Fine-Tuning, Preprocesamiento, Anonimizaci√≥n y Evaluaci√≥n de Alucinaciones

**Juan Sebasti√°n Ram√≠rez Ayala**  
üìö Ingenier√≠a Estad√≠stica ‚Äî Escuela Colombiana de Ingenier√≠a  

**Daniel Felipe Ruiz Berm√∫dez**  
üìö Ingenier√≠a Estad√≠stica ‚Äî Escuela Colombiana de Ingenier√≠a  

## Descripci√≥n general del proyecto
Este proyecto implementa todo el pipeline completo para construir un LLM especializado en di√°logo terap√©utico.  
Incluye limpieza y anonimizaci√≥n del dataset, fine-tuning con LoRA, carga del modelo, interfaz CLI y evaluaci√≥n del modelo evaluando alucinaciones.

## Estructura del repositorio


PsyThera-LLM/
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îÇ
‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îî‚îÄ‚îÄ CLI_interface.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ Copia_de_DLLM_FINPROJECT (1).ipynb
‚îÇ
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îî‚îÄ‚îÄ evaluacion_hallucinations_model.csv
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îÇ
‚îî‚îÄ‚îÄ README.md

---

## 1. Carga del dataset
- Se utiliza KaggleHub para cargar el dataset Therapist Q&A Dataset.
- Exploraci√≥n inicial del dataframe: formas, columnas y valores nulos.

---

## 2. Limpieza del dataset
Se realizan las siguientes acciones:
- Eliminaci√≥n de preguntas y respuestas vac√≠as.
- Eliminaci√≥n de duplicados exactos.
- Limpieza de espacios en blanco y saltos de l√≠nea.
- Normalizaci√≥n de strings.

---

## 3. Anonimizaci√≥n (Reglas + NER)

### A. Anonimizaci√≥n por patrones (RegEx)
Se reemplazan autom√°ticamente:
- URLs ‚Üí [URL]
- Correos ‚Üí [EMAIL]
- Tel√©fonos ‚Üí [PHONE]
- Handles de redes ‚Üí [HANDLE]

### B. Anonimizaci√≥n usando NER (spaCy)
Detecci√≥n de entidades:
- PERSON ‚Üí [PERSON]
- GPE ‚Üí [GPE]
- LOC ‚Üí [LOC]
- ORG ‚Üí [ORG]

Se generan columnas anonimizadas (Question_anon, Answer_anon).

---

## 4. Detecci√≥n de contenido sensible
Se identifican contenidos con riesgo mediante listas de palabras clave:
- Autolesi√≥n / suicidio
- Violencia
- Abuso sexual

Se asigna un nivel de riesgo:
- high
- medium
- none

Y se separan datasets: psych_safe.csv y psych_sensitive.csv.

---

## 5. Filtro por longitud
Se aplican filtros para remover ejemplos demasiado cortos o largos:
- Pregunta: 5‚Äì400 palabras
- Respuesta: 5‚Äì600 palabras

Resultado final: psych_clean_final.csv.

---

## 6. Conversi√≥n a HuggingFace Dataset
El dataset se transforma a formato HuggingFace e incorpora plantilla estilo instruct:


[USER] pregunta
[THERAPIST] respuesta


---
## 7. Tokenizaci√≥n
- Uso de tokenizer de TinyLlama-1.1B-Chat-v1.0
- Tokenizaci√≥n de preguntas y respuestas
- Padding y truncamiento a longitud m√°xima de 1024 tokens
---

## 8. Fine-Tuning del modelo

### A. Phi-3-mini-4k-instruct con LoRA + QLoRA
- Cuantizaci√≥n 4-bit (BitsAndBytes)
- Adaptadores LoRA colocados en capas del modelo
- Entrenamiento con Trainer y guardado de checkpoints

### B. TinyLlama-1.1B-Chat-v1.0
Incluye:
- Collator especializado para entrenar solo la parte de [THERAPIST]
- Tokenizaci√≥n optimizada
- Entrenamiento completo del modelo

---

## 9. Guardado y uso del modelo finetuneado
- El mejor checkpoint se guarda autom√°ticamente.
- Se prepara para uso en Ollama o por medio del pipeline de HuggingFace.

---

## 10. Convertir el modelo entrenado en PyTorch a formato GGUF

Para habilitar la ejecuci√≥n del modelo fine-tuneado dentro de Ollama, fue necesario transformar el checkpoint de HuggingFace (generado por `Trainer`) al formato **GGUF**, compatible con el motor `llama.cpp`. El procedimiento realizado fue el siguiente:

1. **Identificaci√≥n del checkpoint final del modelo**
   - Una vez completado el entrenamiento, se seleccion√≥ el checkpoint correspondiente al mejor desempe√±o.
   - Este checkpoint inclu√≠a los archivos esenciales:
     - `config.json`  
     - `model.safetensors`  
     - `tokenizer.json`, `tokenizer.model`, `tokenizer_config.json`  
   - El directorio del checkpoint ten√≠a la estructura est√°ndar de un modelo HuggingFace.

2. **Clonaci√≥n del repositorio `llama.cpp`**
   - Para usar los scripts de conversi√≥n oficiales, se descarg√≥ el repositorio:
     ```bash
     cd D:/
     git clone https://github.com/ggerganov/llama.cpp
     cd llama.cpp
     ```
   - Este repositorio incluye `convert_hf_to_gguf.py`, necesario para convertir modelos HF a GGUF.

3. **Ejecuci√≥n directa del script de conversi√≥n**
   - En este caso **no fue necesario crear un entorno virtual**, ya que el sistema ya ten√≠a las dependencias requeridas (`transformers`, `safetensors`, etc.).
   - Por tanto, se procedi√≥ directamente a ejecutar la conversi√≥n:
     ```bash
     python convert_hf_to_gguf.py ^
       "D:/.../psych_tinyllama_L/checkpoint-876" ^
       --outfile "D:/psych_tinyllama_L.gguf" ^
       --outtype f16
     ```
   - Se seleccion√≥ `f16` para obtener un modelo m√°s liviano manteniendo buena calidad num√©rica.

4. **Confirmaci√≥n de la exportaci√≥n**
   - El proceso finaliz√≥ mostrando:
     ```
     Model successfully exported to D:\psych_tinyllama_L.gguf
     ```
   - Esto verific√≥ que el modelo hab√≠a sido convertido correctamente.

5. **Alojamiento del archivo `.gguf` dentro del directorio de Ollama**
   - Tras generar el archivo GGUF, este tuvo que ser **copiado manualmente** al directorio donde Ollama almacena sus modelos, el cual normalmente incluye las subcarpetas:
     ```
     C:\Users\<usuario>\.ollama\models\
     ```
   - Esto garantiza que Ollama pueda encontrar el modelo y usarlo.

6. **Creaci√≥n del `Modelfile`**
   - Una vez colocado el `.gguf` dentro del directorio de modelos de Ollama, se cre√≥ un archivo llamado `Modelfile` en la misma ubicaci√≥n.  
   - Ese archivo indica a Ollama:
     - cu√°l es el archivo base del modelo (el `.gguf`)
     - cu√°l es el template de interacci√≥n
     - y los par√°metros de generaci√≥n
   - Ejemplo del `Modelfile` utilizado:
     ```
     FROM ./psych_tinyllama_L.gguf

     TEMPLATE """
     <|system|>
     You are a helpful therapist AI. Act kindly and professionally.

     <|user|>
     {{ .Prompt }}

     <|assistant|>
     """

     PARAMETER temperature 0.7
     PARAMETER top_p 0.9
     ```

7. **Construcci√≥n final del modelo en Ollama**
   - Con el `Modelfile` listo, se ejecut√≥:
     ```bash
     ollama create psych-therapist -f Modelfile
     ```
   - Esto gener√≥ un modelo instalable y ejecutable en Ollama bajo el nombre `psych-therapist`.

---

En s√≠ntesis, el proceso completa la transformaci√≥n desde un checkpoint de PyTorch a un modelo `.gguf` compatible con Ollama, aloj√°ndolo en el directorio correspondiente y configurando su comportamiento mediante un `Modelfile`.


---

## 11. Interfaz CLI
Se incluye funci√≥n generadora de prompts:


[USER] <pregunta>
[THERAPIST]


Y se habilita inferencia desde la l√≠nea de comandos con el modelo entrenado.

---




## 11. Evaluaci√≥n de alucinaciones
Se eval√∫a el modelo usando 4 tipos de preguntas:
- Psicolog√≠a (esperadas)
- Fuera de contexto (para detectar alucinaci√≥n)
- Sensibles √©ticamente
- Control general

Se genera archivo final:
- evaluacion_hallucinations_model.csv

Cada fila contiene:
- Tipo de pregunta
- Pregunta
- Respuesta del modelo
- Columna para calificaci√≥n manual

---



